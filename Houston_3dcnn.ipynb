{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Conv3D, MaxPooling3D, ZeroPadding3D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, Input\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.optimizers import Nadam, SGD, Adadelta, RMSprop\n",
    "from keras.optimizers import adam_v2\n",
    "adam = adam_v2.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "import keras.callbacks as kcallbacks\n",
    "from keras.regularizers import l2\n",
    "import time\n",
    "import collections\n",
    "import pandas as pd\n",
    "from sklearn import metrics, preprocessing\n",
    "from Utils import zeroPadding, normalization, modelStatsRecord, averageAccuracy, cnn_3D,densenet_IN, densenet_IN_no_bottleneck_layer\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# 产生新数据集的过程\n",
    "# indexToAssignment(train_indices, whole_data.shape[0], whole_data.shape[1], PATCH_LENGTH)\n",
    "# 训练集 ，151 ，151， 3\n",
    "def indexToAssignment(index_, Row, Col, pad_length):\n",
    "    new_assign = {}\n",
    "    for counter, value in enumerate(index_):\n",
    "        assign_0 = value // Col + pad_length\n",
    "        assign_1 = value % Col + pad_length\n",
    "        new_assign[counter] = [assign_0, assign_1]\n",
    "    return new_assign\n",
    "\n",
    "def assignmentToIndex(assign_0, assign_1, Row, Col):\n",
    "    new_index = assign_0 * Col + assign_1\n",
    "    return new_index\n",
    "\n",
    "def selectNeighboringPatch(matrix, pos_row, pos_col, ex_len):\n",
    "    selected_rows = matrix[range(pos_row - ex_len, pos_row + ex_len + 1), :]\n",
    "    selected_patch = selected_rows[:, range(pos_col - ex_len, pos_col + ex_len + 1)]\n",
    "    return selected_patch\n",
    "\n",
    "# divide dataset into train and test datasets\n",
    "def sampling(proptionVal, groundTruth):\n",
    "    labels_loc = {}\n",
    "    train = {}\n",
    "    test = {}\n",
    "    m = max(groundTruth)\n",
    "    print(m)\n",
    "    # 16\n",
    "    for i in range(m):\n",
    "        indices = [j for j, x in enumerate(groundTruth.ravel().tolist()) if x == i + 1]\n",
    "        np.random.shuffle(indices)\n",
    "        labels_loc[i] = indices\n",
    "        nb_val = int(proptionVal * len(indices))\n",
    "        train[i] = indices[:-nb_val]\n",
    "        test[i] = indices[-nb_val:]\n",
    "        \n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(m):\n",
    "        train_indices += train[i]\n",
    "        test_indices += test[i]\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    print(len(test_indices))\n",
    "    # 8194\n",
    "    print(len(train_indices))\n",
    "    # 2055\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "def model_DenseNet():\n",
    "    model_dense = cnn_3D.ResnetBuilder.build_resnet_8((1,img_rows, img_cols, img_channels), nb_classes)\n",
    "    #model_dense = densenet_IN.ResnetBuilder.build_resnet_8((1, img_rows, img_cols, img_channels), nb_classes)\n",
    "\n",
    "    RMS = RMSprop(lr=0.0001)#0.0003\n",
    "    # Let's train the model using RMSprop\n",
    "    model_dense.compile(loss='categorical_crossentropy', optimizer=RMS, metrics=['accuracy'])\n",
    "\n",
    "    return model_dense\n",
    "\n",
    "\n",
    "def dimension_PCA(data, input_dimension):\n",
    "    dt = np.array(data,dtype='float32').reshape(-1,200)\n",
    "    pca = PCA(n_components=input_dimension)\n",
    "    dt = pca.fit_transform(dt)#data  \n",
    "    whole_pca = np.zeros((data.shape[0], data.shape[1], input_dimension))\n",
    "\n",
    "\n",
    "    for i in range(input_dimension):\n",
    "         whole_pca[:, :, i] = dt[:,i].reshape(data.shape[0], data.shape[1])\n",
    "\n",
    "\n",
    "    print (whole_pca.shape))\n",
    "\n",
    "    return whole_pca\n",
    "\n",
    "\n",
    "def dimension_AE(data,input_dimension):\n",
    "    [d1,d2,d3]=data.shape\n",
    "    data=data.reshape([d1*d2,d3])\n",
    "\n",
    "    mean=data.mean()\n",
    "    std=data.std()\n",
    "    data=(data-mean)/std\n",
    "\n",
    "\n",
    "    input_data = Input(shape=(200,))\n",
    "    encoded = Dense(128,activation = 'relu')(input_data)\n",
    "    encoded = Dense(input_dimension, activation='relu')(encoded)\n",
    "\n",
    "    encoder = Model(input_data,encoded)\n",
    "    encoder.summary()\n",
    "    encoded_data = pd.DataFrame(encoder.predict(data))\n",
    "    print(encoded_data.shape)\n",
    "    \n",
    "    whole_ae = np.zeros((d1, d2, input_dimension))\n",
    "\n",
    "    for i in range(input_dimension):\n",
    "         whole_ae[:, :, i] = encoded_data.values[:,i].reshape(d1, d2)\n",
    "\n",
    "    print(whole_ae.shape)\n",
    "    return whole_ae\n",
    "\n",
    "\n",
    "\n",
    "mat_data = sio.loadmat('Houston.mat')\n",
    "data_IN = mat_data['Houston']\n",
    "#data_IN = dimension_PCA(data_IN,108)\n",
    "\n",
    "mat_gt = sio.loadmat('Houston_gt.mat')\n",
    "gt_IN = mat_gt['Houston_gt']\n",
    "# print('data_IN:',data_IN)\n",
    "\n",
    "#reduce dimension\n",
    "#data_IN = dimension_PCA(data_IN,108)  #pca\n",
    "#data_IN = dimension_AE(data_IN,108) #ae\n",
    "\n",
    "print(data_IN.shape)\n",
    "# (145,145,200)\n",
    "print(gt_IN.shape)\n",
    "# (145,145)\n",
    "\n",
    "# new_gt_IN = set_zeros(gt_IN, [1,4,7,9,13,15,16])\n",
    "new_gt_IN = gt_IN\n",
    "\n",
    "batch_size = 8\n",
    "nb_classes = 15\n",
    "nb_epoch = 100 # 400\n",
    "img_rows, img_cols = 7, 7  # 27, 27\n",
    "patience = 200  #200\n",
    "\n",
    "INPUT_DIMENSION_CONV = 144\n",
    "INPUT_DIMENSION = 144\n",
    "\n",
    "\n",
    "\n",
    "TOTAL_SIZE =15029 \n",
    "VAL_SIZE = 1503\n",
    "\n",
    "TRAIN_SIZE = 7518\n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "VALIDATION_SPLIT = 0.5  # 50% for trainnig and 50% for validation and testing\n",
    "\n",
    "\n",
    "img_channels = 144\n",
    "PATCH_LENGTH = 3  # Patch_size (13*2+1)*(13*2+1)\n",
    "\n",
    "print(data_IN.shape[:2])\n",
    "\n",
    "print(np.prod(data_IN.shape[:2]))\n",
    "\n",
    "print(data_IN.shape[2:])\n",
    "\n",
    "print(np.prod(data_IN.shape[2:]))\n",
    "\n",
    "print(np.prod(new_gt_IN.shape[:2]))\n",
    "\n",
    "\n",
    "\n",
    "data = data_IN.reshape(np.prod(data_IN.shape[:2]), np.prod(data_IN.shape[2:]))\n",
    "gt = new_gt_IN.reshape(np.prod(new_gt_IN.shape[:2]), )\n",
    "\n",
    "\n",
    "data = preprocessing.scale(data)\n",
    "print(data.shape)\n",
    "# (21025, 200)\n",
    "\n",
    "# scaler = preprocessing.MaxAbsScaler()\n",
    "# data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "data_ = data.reshape(data_IN.shape[0], data_IN.shape[1], data_IN.shape[2])\n",
    "whole_data = data_\n",
    "padded_data = zeroPadding.zeroPadding_3D(whole_data, PATCH_LENGTH)\n",
    "print(padded_data.shape)\n",
    "# (151, 151, 200)\n",
    "\n",
    "\n",
    "ITER = 1\n",
    "CATEGORY = 15\n",
    "\n",
    "train_data = np.zeros((TRAIN_SIZE, 2 * PATCH_LENGTH + 1, 2 * PATCH_LENGTH + 1, INPUT_DIMENSION_CONV))\n",
    "print(train_data.shape)\n",
    "# (2055, 7, 7, 200)\n",
    "test_data = np.zeros((TEST_SIZE, 2 * PATCH_LENGTH + 1, 2 * PATCH_LENGTH + 1, INPUT_DIMENSION_CONV))\n",
    "print(test_data.shape)\n",
    "# (8194, 7, 7, 200)\n",
    "\n",
    "\n",
    "KAPPA_3DCNN = []\n",
    "OA_3DCNN = []\n",
    "AA_3DCNN = []\n",
    "TRAINING_TIME_3DCNN = []\n",
    "TESTING_TIME_3DCNN = []\n",
    "ELEMENT_ACC_3DCNN = np.zeros((ITER, CATEGORY))\n",
    "\n",
    "#seeds = [1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229]\n",
    "\n",
    "seeds = [1334]\n",
    "\n",
    "for index_iter in range(ITER):\n",
    "    print(\"# %d Iteration\" % (index_iter + 1))\n",
    "    # # 1 Iteration\n",
    "\n",
    "    # save the best validated model\n",
    "    best_weights_DenseNet_path = 'weights/IN/Houston_bestamdschaha_3DCNN514_' + str(\n",
    "        index_iter + 1) + '.hdf5'\n",
    "\n",
    "    # Get test and training samples through the sampling function.\n",
    "    np.random.seed(seeds[index_iter])\n",
    "    train_indices, test_indices = sampling(VALIDATION_SPLIT, gt)\n",
    "    # train_indices 2055     test_indices 8094\n",
    "\n",
    "\n",
    "    y_train = gt[train_indices] - 1\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "\n",
    "    y_test = gt[test_indices] - 1\n",
    "    y_test = to_categorical(np.asarray(y_test))\n",
    "\n",
    " \n",
    "    train_assign = indexToAssignment(train_indices, whole_data.shape[0], whole_data.shape[1], PATCH_LENGTH)\n",
    "    for i in range(len(train_assign)):\n",
    "        train_data[i] = selectNeighboringPatch(padded_data, train_assign[i][0], train_assign[i][1], PATCH_LENGTH)\n",
    "\n",
    "    test_assign = indexToAssignment(test_indices, whole_data.shape[0], whole_data.shape[1], PATCH_LENGTH)\n",
    "    for i in range(len(test_assign)):\n",
    "        test_data[i] = selectNeighboringPatch(padded_data, test_assign[i][0], test_assign[i][1], PATCH_LENGTH)\n",
    "\n",
    "\n",
    "    x_train = train_data.reshape(train_data.shape[0], train_data.shape[1], train_data.shape[2], INPUT_DIMENSION_CONV)\n",
    "    x_test_all = test_data.reshape(test_data.shape[0], test_data.shape[1], test_data.shape[2], INPUT_DIMENSION_CONV)\n",
    "\n",
    "\n",
    "    x_val = x_test_all[-VAL_SIZE:]\n",
    "    y_val = y_test[-VAL_SIZE:]\n",
    "\n",
    "    x_test = x_test_all[:-VAL_SIZE]\n",
    "    y_test = y_test[:-VAL_SIZE]\n",
    "\n",
    "    ############################################################################################################\n",
    "\n",
    "    model_densenet = model_DenseNet()\n",
    "\n",
    "\n",
    "    earlyStopping6 = kcallbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='auto')\n",
    "    # Users will save the model at the end of each epoch. If save_best_only=True, the last data with the latest verification error will be saved.\n",
    "    saveBestModel6 = kcallbacks.ModelCheckpoint(best_weights_DenseNet_path, monitor='val_loss', verbose=1,\n",
    "                                                save_best_only=True,\n",
    "                                                mode='auto')\n",
    "\n",
    "    # Training and verification\n",
    "    tic6 = time.perf_counter()\n",
    "    print(x_train.shape, x_test.shape)\n",
    "    # (2055,7,7,200)  (7169,7,7,200)\n",
    "    history_3d_densenet = model_densenet.fit(\n",
    "        x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], x_train.shape[3], 1), y_train,\n",
    "        validation_data=(x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2], x_val.shape[3], 1), y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=nb_epoch, shuffle=True, callbacks=[earlyStopping6, saveBestModel6])\n",
    "    toc6 = time.perf_counter()\n",
    " \n",
    "    # test\n",
    "    tic7 = time.perf_counter()\n",
    "    loss_and_metrics = model_densenet.evaluate(\n",
    "        x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], x_test.shape[3], 1), y_test,\n",
    "        batch_size=batch_size)\n",
    "    toc7 = time.perf_counter()\n",
    "\n",
    "    print('3DCNN Fit Time: ', toc6 - tic6)\n",
    "    print('3DCNN Test time:', toc7 - tic7)\n",
    "\n",
    "    print('3DCNN Test score:', loss_and_metrics[0])\n",
    "    print('3DCNN Test accuracy:', loss_and_metrics[1])\n",
    "\n",
    "    # print(history_3d_densenet.history.keys())\n",
    "    # dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
    "\n",
    "    # predict\n",
    "    pred_test = model_densenet.predict(\n",
    "        x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], x_test.shape[3], 1)).argmax(axis=1)\n",
    "    \n",
    "    collections.Counter(pred_test)\n",
    "\n",
    "    gt_test = gt[test_indices] - 1\n",
    "    # print(len(gt_test))\n",
    "    # 8194\n",
    "    overall_acc = metrics.accuracy_score(pred_test, gt_test[:-VAL_SIZE])\n",
    "    confusion_matrix = metrics.confusion_matrix(pred_test, gt_test[:-VAL_SIZE])\n",
    "    print(confusion_matrix)\n",
    "    print(\"3DCNN finished.\")\n",
    "    print(\"# %d Iteration\" % (index_iter + 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
